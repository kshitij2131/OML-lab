{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Lab5\n",
        "##Optimization in Machine Learning (Fall Semester)\n",
        "###kshitij jaiswal (B20CS028)"
      ],
      "metadata": {
        "id": "mU53SmZ_MlY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8hc13CYMjjQ",
        "outputId": "ae2525d4-8ad3-4a23-e2f8-b8ce96578fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimal solution for x (using steepest descent method):  [[7.96937268]\n",
            " [3.98467358]]\n",
            "value of F(x_k) : 8.805558873740041e-07\n",
            "norm of grad(f(x)) : 8.215347703527154e-05\n",
            "number of function calls : 7656\n",
            "number of gradient of function calls : 1263\n",
            "number of iterations :  631\n"
          ]
        }
      ],
      "source": [
        "#q1\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg\n",
        "\n",
        "\n",
        "r = 8.00\n",
        "delta = 0.01\n",
        "maxIter = 5000\n",
        "alpha = 1.00\n",
        "r0 = 0.50\n",
        "beta1 = 1e-4\n",
        "beta2 = 0.9\n",
        "epsilon = 1e-4\n",
        "\n",
        "gradEvals = 0\n",
        "funcEvals = 0\n",
        "\n",
        "\n",
        "def fun(params):\n",
        "    global funcEvals\n",
        "    funcEvals += 1\n",
        "\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return ((x1 - r)**4) + ((x1 - 2*x2)**2)\n",
        "\n",
        "def gradFx(params, fx):\n",
        "    global gradEvals\n",
        "    gradEvals += 1\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "    F = fx(params)\n",
        "\n",
        "    h, g = 1e-5, np.zeros(numParams)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        f = fx(params)\n",
        "        g[i] = (f - F)/h\n",
        "        params[i][0] -= h\n",
        "\n",
        "    return g.reshape(-1, 1)\n",
        "\n",
        "\n",
        "def armijo(xk, alphak, dk, gradFxk):\n",
        "    lhs = fun(xk + alphak*dk)\n",
        "    rhs = fun(xk) + alphak*beta1*np.dot(gradFxk.T, dk)\n",
        "    if lhs <= rhs:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def wolfe(xk, alphak, dk, gradFxk):\n",
        "    lhs = np.dot(gradFx(xk + alphak*dk, fun).T, dk)\n",
        "    rhs = beta2*np.dot(gradFxk.T, dk)\n",
        "    if lhs >= rhs:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    params = np.array([[r-1], [r+1]])\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "\n",
        "    #using inexact line search algorithm along with steepest descent..\n",
        "\n",
        "    iter = 0\n",
        "    xk = params\n",
        "    gradFxk = gradFx(xk, fun)\n",
        "\n",
        "    while linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "        dk = -1 * gradFxk\n",
        "\n",
        "        alpha = 1.00\n",
        "        while armijo(xk, alpha, dk, gradFxk) == False or wolfe(xk, alpha, dk, gradFxk) == False:\n",
        "            alpha = alpha*r0\n",
        "\n",
        "        xk = xk + alpha*dk\n",
        "        gradFxk = gradFx(xk, fun)\n",
        "        iter += 1\n",
        "\n",
        "    print(\"optimal solution for x (using steepest descent method): \", xk)\n",
        "    print(\"value of F(x_k) :\", fun(xk))\n",
        "    print(\"norm of grad(f(x)) :\", np.linalg.norm(gradFxk))\n",
        "    print(\"number of function calls :\", funcEvals)\n",
        "    print(\"number of gradient of function calls :\", gradEvals)\n",
        "    print(\"number of iterations : \", iter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#q2\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg\n",
        "\n",
        "\n",
        "r = 8.00\n",
        "delta = 0.01\n",
        "maxIter = 5000\n",
        "alpha = 1.00\n",
        "r0 = 0.50\n",
        "beta1 = 1e-4\n",
        "beta2 = 0.9\n",
        "epsilon = 1e-4\n",
        "gradEvals = 0\n",
        "funcEvals = 0\n",
        "B = np.array([[2*r, np.sqrt(r)], [np.sqrt(r), r]])\n",
        "\n",
        "\n",
        "def fun(params):\n",
        "    global funcEvals\n",
        "    funcEvals += 1\n",
        "\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return ((x1 - r)**4) + ((x1 - 2*x2)**2)\n",
        "\n",
        "def gradFx(params, fx):\n",
        "    global gradEvals\n",
        "    gradEvals += 1\n",
        "\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "    F = fx(params)\n",
        "\n",
        "    h, g = 1e-5, np.zeros(numParams)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        f = fx(params)\n",
        "        g[i] = (f - F)/h\n",
        "        params[i][0] -= h\n",
        "\n",
        "    return g.reshape(-1, 1)\n",
        "\n",
        "def armijo(xk, alphak, dk, gradFxk):\n",
        "    lhs = fun(xk + alphak*dk)\n",
        "    rhs = fun(xk) + alphak*beta1*np.dot(gradFxk.T, dk)\n",
        "    if lhs <= rhs:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def wolfe(xk, alphak, dk, gradFxk):\n",
        "    lhs = np.dot(gradFx(xk + alphak*dk, fun).T, dk)\n",
        "    rhs = beta2*np.dot(gradFxk.T, dk)\n",
        "    if lhs >= rhs:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    params = np.array([[r-1], [r+1]])\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "\n",
        "    #using inexact line search algorithm along with bergman's distance..\n",
        "\n",
        "    iter = 0\n",
        "    xk = params\n",
        "    gradFxk = gradFx(xk, fun)\n",
        "\n",
        "\n",
        "\n",
        "    while linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "        dk = -2 * np.linalg.solve(B, gradFxk)\n",
        "\n",
        "        alpha = 1.00\n",
        "        while armijo(xk, alpha, dk, gradFxk) == False or wolfe(xk, alpha, dk, gradFxk) == False:\n",
        "            alpha = alpha*r0\n",
        "\n",
        "        xk = xk + alpha*dk\n",
        "        gradFxk = gradFx(xk, fun)\n",
        "        iter += 1\n",
        "\n",
        "    print(\"optimal solution for x (using mirror descent): \", xk)\n",
        "    print(\"norm of grad(f(x)) :\", np.linalg.norm(gradFxk))\n",
        "    print(\"value of F(x_k) :\", fun(xk))\n",
        "    print(\"number of function calls :\", funcEvals)\n",
        "    print(\"number of gradient of function calls :\", gradEvals)\n",
        "    print(\"number of iterations : \", iter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC8fPlbqNevV",
        "outputId": "53a06063-98c1-426b-98aa-d5b9aa9e97d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimal solution for x (using mirror descent):  [[8.01550235]\n",
            " [4.00775255]]\n",
            "norm of grad(f(x)) : 5.456705329675777e-05\n",
            "value of F(x_k) : 5.776262859093008e-08\n",
            "number of function calls : 164\n",
            "number of gradient of function calls : 33\n",
            "number of iterations :  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Bregman function h(x) = 1/2(x^TBx), we obtain an improvement of converging in 16 iterations compared to 600 iterations in steepest gradient descent.."
      ],
      "metadata": {
        "id": "PmoI9jPUOGUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#q3\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg\n",
        "\n",
        "\n",
        "r = 8.00\n",
        "delta = 0.01\n",
        "maxIter = 500\n",
        "epsilon = 1e-4\n",
        "\n",
        "\n",
        "def fun1(params):\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return (x1**2) + (x2**2) - 2\n",
        "\n",
        "def fun2(params):\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return (np.e)**(x1-1) + (x2**3) - 2\n",
        "\n",
        "def gradFx(params, fx):\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "    F = fx(params)\n",
        "\n",
        "    h, g = 1e-5, np.zeros(numParams)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        f = fx(params)\n",
        "        g[i] = (f - F)/h\n",
        "        params[i][0] -= h\n",
        "\n",
        "    return g.reshape(-1, 1)\n",
        "\n",
        "def jacobianFx(params):\n",
        "    gradFx1 = gradFx(params, fun1)\n",
        "    gradFx2 = gradFx(params, fun2)\n",
        "\n",
        "    j = np.vstack((gradFx1.T, gradFx2.T))\n",
        "    return j\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    params = np.array([[2+(r/10)], [2+(r/10)]])\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "\n",
        "    #using newton's method..\n",
        "\n",
        "    iter = 0\n",
        "    xk = params\n",
        "    Fxk = np.vstack((fun1(xk), fun2(xk)))\n",
        "    jacFxk = jacobianFx(xk)\n",
        "\n",
        "\n",
        "    while linalg.norm(Fxk) >= epsilon and iter < maxIter:\n",
        "        dk = -np.linalg.solve(jacFxk, Fxk)\n",
        "        xk = xk + dk\n",
        "        Fxk = np.vstack((fun1(xk), fun2(xk)))\n",
        "        jacFxk = jacobianFx(xk)\n",
        "        iter += 1\n",
        "\n",
        "    print(\"optimal solution for x (using newton's method): \", xk)\n",
        "    print(\"norm of F(x) :\", np.linalg.norm(Fxk))\n",
        "    print(\"number of iterations : \", iter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lkrJkX4NhqE",
        "outputId": "d4d5164b-8c58-41be-e1b0-4385b134e7c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimal solution for x (using newton's method):  [[0.99999899]\n",
            " [1.00000191]]\n",
            "norm of F(x) : 5.0458860617510705e-06\n",
            "number of iterations :  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Solutions -  (1, 1) & (-0.71, 1.22)"
      ],
      "metadata": {
        "id": "ahd8W3V9Ojgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#q4\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg\n",
        "\n",
        "\n",
        "r = 8.00\n",
        "delta = 0.01\n",
        "maxIter = 500\n",
        "epsilon = 1e-4\n",
        "\n",
        "\n",
        "def fun(params):\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return ((x1 - r)**4) + ((x1 - 2*x2)**2)\n",
        "\n",
        "def gradFun1(params):\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return 4*((x1 - r)**3) + 2*(x1 - 2*x2)\n",
        "\n",
        "def gradFun2(params):\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return (-4)*(x1 - 2*x2)\n",
        "\n",
        "\n",
        "def gradFx(params, fx):\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "    F = fx(params)\n",
        "\n",
        "    h, g = 1e-5, np.zeros(numParams)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        f = fx(params)\n",
        "        g[i] = (f - F)/h\n",
        "        params[i][0] -= h\n",
        "\n",
        "    return g.reshape(-1, 1)\n",
        "\n",
        "def jacobianFx(params):\n",
        "    gradFx1 = gradFx(params, gradFun1)\n",
        "    gradFx2 = gradFx(params, gradFun2)\n",
        "\n",
        "    j = np.vstack((gradFx1.T, gradFx2.T))\n",
        "    return j\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    params = np.array([[r+1], [r-1]])\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "\n",
        "    #using newton's method..\n",
        "\n",
        "    iter = 0\n",
        "    xk = params\n",
        "    gradFxk = np.vstack((gradFun1(xk), gradFun2(xk)))\n",
        "    jacGradFxk = jacobianFx(xk)\n",
        "\n",
        "\n",
        "    while linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "        dk = -linalg.solve(jacGradFxk, gradFxk)\n",
        "        xk = xk + dk\n",
        "        gradFxk = np.vstack((gradFun1(xk), gradFun2(xk)))\n",
        "        jacGradFxk = jacobianFx(xk)\n",
        "        iter += 1\n",
        "\n",
        "    print(\"optimal solution for x (using newton's method): \", xk)\n",
        "    print(\"minimum value of F(x) :\", fun(xk))\n",
        "    print(\"norm of grad(F(x)) :\", linalg.norm(gradFxk))\n",
        "    print(\"number of iterations : \", iter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfS3-9nqNkcP",
        "outputId": "9f7983a6-459d-46b2-f708-a9e07f2b69dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimal solution for x (using newton's method):  [[8.02602203]\n",
            " [4.01301102]]\n",
            "minimum value of F(x) : 4.585270290415445e-07\n",
            "norm of grad(F(x)) : 7.048288906815923e-05\n",
            "number of iterations :  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#q5\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg\n",
        "from cvxopt import solvers, matrix\n",
        "\n",
        "r = 8.00\n",
        "delta = 0.01\n",
        "maxIter = 500\n",
        "epsilon = 1e-4\n",
        "pi = np.pi\n",
        "n = 30\n",
        "\n",
        "def I():\n",
        "    I1 = []\n",
        "    for i in range(2, n+1):\n",
        "        if (i&1) > 0:\n",
        "            I1.append(i)\n",
        "\n",
        "    I2 = []\n",
        "    for i in range(2, n+1):\n",
        "        if (i&1) == 0:\n",
        "            I2.append(i)\n",
        "\n",
        "    return (I1, I2)\n",
        "\n",
        "I1, I2 = I()\n",
        "\n",
        "def fun1(params):\n",
        "    x1 = params[0][0]\n",
        "\n",
        "    t = 0.0\n",
        "    for i in I1:\n",
        "        t += ((params[i-1][0] - np.sin(6*pi*x1 + i*pi/n))**2)\n",
        "\n",
        "    t *= (2/linalg.norm(I1))\n",
        "    t += x1\n",
        "\n",
        "    return t\n",
        "\n",
        "def fun2(params):\n",
        "    x1 = params[0][0]\n",
        "\n",
        "    t = 0.0\n",
        "    for i in I2:\n",
        "        t += ((params[i-1][0] - np.sin(6*pi*x1 + i*pi/n))**2)\n",
        "\n",
        "    t *= (2/linalg.norm(I2))\n",
        "    t -= np.sqrt(x1)\n",
        "    t += 1\n",
        "\n",
        "    return t\n",
        "\n",
        "def fun(params):\n",
        "    return (r/10)*fun1(params) + (1-(r/10))*fun2(params)\n",
        "\n",
        "\n",
        "def gradFx(params, fx):\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "    F = fx(params)\n",
        "    h, g = 1e-5, np.zeros(numParams)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        f = fx(params)\n",
        "        g[i] = (f - F)/h\n",
        "        params[i][0] -= h\n",
        "\n",
        "    return g.reshape(-1, 1)\n",
        "\n",
        "def hessianFx(params, fx):\n",
        "    h = 1e-5\n",
        "    numParams = params.shape[0]\n",
        "    H = np.matrix(np.zeros((numParams, numParams)))\n",
        "    F = fx(params)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        fxi = fx(params)\n",
        "\n",
        "        for j in range(i+1):\n",
        "            params[j][0] += h\n",
        "\n",
        "            params[i][0] -= h\n",
        "            fxj = fx(params)\n",
        "            params[i][0] += h\n",
        "\n",
        "            H[i,j] = (fx(params) - fxi - fxj + F)/(h**2)\n",
        "            H[j,i] = H[i,j]\n",
        "            params[j][0] -= h\n",
        "        params[i][0] -= h\n",
        "\n",
        "\n",
        "    #adding the regularization parameter..\n",
        "    lmbda = min(linalg.eig(H)[0])\n",
        "    if lmbda > delta:\n",
        "        lmbda = 0\n",
        "    else:\n",
        "        lmbda = delta-lmbda\n",
        "    for i in range(numParams):\n",
        "        H[i,i] += lmbda\n",
        "\n",
        "\n",
        "\n",
        "    return H\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    params = np.zeros((n, 1))\n",
        "    numParams = params.shape[0]\n",
        "    for i in range(numParams):\n",
        "        if i == 0:\n",
        "            params[i][0] = np.random.uniform(0.001, 1.0)\n",
        "        else:\n",
        "            params[i][0] = np.random.uniform(-1.0, 1.0)\n",
        "\n",
        "\n",
        "    #using modified newton's method..\n",
        "\n",
        "    iter = 0\n",
        "    xk = params\n",
        "    gradFxk = gradFx(xk, fun)\n",
        "    hessFxk = hessianFx(xk, fun)\n",
        "    A = np.vstack((-np.identity(numParams), np.identity(numParams)))\n",
        "\n",
        "    while linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "        b = np.append(xk-(-1.0), 1.0-xk)\n",
        "        b[0] += (-1.0)\n",
        "        b[0] -= 0.001\n",
        "\n",
        "        sol = solvers.qp(matrix(hessFxk, tc = 'd'), matrix(gradFxk), matrix(A), matrix(b))\n",
        "        dk = np.array(sol['x'])\n",
        "        xk = xk + dk\n",
        "\n",
        "        gradFxk = gradFx(xk, fun)\n",
        "        hessFxk = hessianFx(xk, fun)\n",
        "        iter += 1\n",
        "\n",
        "    print(\"optimal solution for x (using modified newton's method): \", xk)\n",
        "    print(\"minimum value of F(x) :\", fun(xk))\n",
        "    print(\"norm of grad(F(x)) :\", linalg.norm(gradFxk))\n",
        "    print(\"number of iterations : \", iter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRluFS26Nohd",
        "outputId": "ad74a529-8799-4c5b-ac06-babcc78ced33"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  3.9465e-01 -9.5414e+01  2e+02  9e-01  8e-16\n",
            " 1:  8.4415e-01 -2.6652e+01  3e+01  2e-16  2e-15\n",
            " 2: -1.5032e-01 -1.9421e+00  2e+00  2e-16  1e-15\n",
            " 3: -2.3420e-01 -2.7495e-01  4e-02  2e-16  4e-16\n",
            " 4: -2.3628e-01 -2.3736e-01  1e-03  2e-16  5e-16\n",
            " 5: -2.3632e-01 -2.3634e-01  2e-05  1e-16  4e-17\n",
            " 6: -2.3632e-01 -2.3632e-01  2e-07  2e-16  2e-16\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -3.9824e+00 -9.8428e+01  3e+02  2e+00  8e-16\n",
            " 1: -1.4412e+00 -4.0981e+01  4e+01  3e-16  1e-15\n",
            " 2: -2.6841e+00 -3.8281e+00  1e+00  1e-16  7e-16\n",
            " 3: -2.7515e+00 -2.7874e+00  4e-02  2e-16  2e-16\n",
            " 4: -2.7563e+00 -2.7588e+00  2e-03  2e-16  1e-16\n",
            " 5: -2.7565e+00 -2.7566e+00  1e-04  2e-16  7e-16\n",
            " 6: -2.7565e+00 -2.7565e+00  1e-06  2e-16  2e-16\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -1.5861e-01 -6.0528e+01  6e+01  5e-17  1e-15\n",
            " 1: -1.7436e-01 -1.9917e+00  2e+00  2e-16  6e-16\n",
            " 2: -3.1828e-01 -5.4740e-01  2e-01  2e-16  2e-16\n",
            " 3: -3.6590e-01 -3.9092e-01  3e-02  2e-16  4e-16\n",
            " 4: -3.7104e-01 -3.7321e-01  2e-03  2e-16  6e-16\n",
            " 5: -3.7151e-01 -3.7171e-01  2e-04  2e-16  9e-16\n",
            " 6: -3.7155e-01 -3.7157e-01  2e-05  2e-16  1e-15\n",
            " 7: -3.7156e-01 -3.7156e-01  2e-06  2e-16  5e-17\n",
            " 8: -3.7156e-01 -3.7156e-01  2e-07  2e-16  3e-16\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  2.1168e-01 -6.0169e+01  6e+01  5e-17  2e-15\n",
            " 1:  1.9532e-01 -1.6190e+00  2e+00  2e-16  1e-15\n",
            " 2:  5.1435e-02 -1.7741e-01  2e-01  2e-16  1e-16\n",
            " 3:  3.8969e-03 -2.1055e-02  2e-02  2e-16  2e-16\n",
            " 4: -1.2520e-03 -3.4253e-03  2e-03  2e-16  5e-17\n",
            " 5: -1.7291e-03 -1.9294e-03  2e-04  2e-16  8e-17\n",
            " 6: -1.7732e-03 -1.7910e-03  2e-05  2e-16  2e-17\n",
            " 7: -1.7775e-03 -1.7793e-03  2e-06  2e-16  6e-17\n",
            " 8: -1.7778e-03 -1.7780e-03  2e-07  2e-16  8e-17\n",
            " 9: -1.7779e-03 -1.7779e-03  7e-09  2e-16  2e-16\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  2.2367e-01 -6.0219e+01  6e+01  5e-17  2e-15\n",
            " 1:  2.0537e-01 -1.6502e+00  2e+00  2e-16  1e-15\n",
            " 2:  5.4558e-02 -1.8422e-01  2e-01  2e-16  1e-16\n",
            " 3:  4.8536e-03 -2.0827e-02  3e-02  2e-16  5e-17\n",
            " 4: -5.1897e-04 -2.8387e-03  2e-03  2e-16  9e-17\n",
            " 5: -1.0280e-03 -1.2390e-03  2e-04  2e-16  8e-18\n",
            " 6: -1.0757e-03 -1.0954e-03  2e-05  2e-16  1e-16\n",
            " 7: -1.0804e-03 -1.0821e-03  2e-06  2e-16  8e-17\n",
            " 8: -1.0810e-03 -1.0812e-03  2e-07  1e-16  8e-18\n",
            " 9: -1.0811e-03 -1.0811e-03  1e-08  1e-16  1e-16\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  2.4120e-01 -6.0274e+01  6e+01  4e-17  2e-15\n",
            " 1:  2.2054e-01 -1.6894e+00  2e+00  1e-16  1e-15\n",
            " 2:  5.8316e-02 -1.9275e-01  3e-01  2e-16  5e-16\n",
            " 3:  5.8890e-03 -2.0684e-02  3e-02  1e-16  2e-16\n",
            " 4:  2.9825e-04 -2.1089e-03  2e-03  2e-16  3e-17\n",
            " 5: -2.2219e-04 -4.4424e-04  2e-04  2e-16  3e-17\n",
            " 6: -2.7015e-04 -2.9052e-04  2e-05  2e-16  6e-17\n",
            " 7: -2.7471e-04 -2.7651e-04  2e-06  2e-16  5e-17\n",
            " 8: -2.7522e-04 -2.7541e-04  2e-07  2e-16  7e-17\n",
            " 9: -2.7528e-04 -2.7530e-04  2e-08  1e-16  7e-17\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  2.4926e-01 -6.0312e+01  6e+01  5e-17  2e-15\n",
            " 1:  2.2750e-01 -1.7060e+00  2e+00  2e-16  1e-15\n",
            " 2:  5.9709e-02 -1.9606e-01  3e-01  1e-16  1e-16\n",
            " 3:  6.1884e-03 -2.0717e-02  3e-02  2e-16  1e-16\n",
            " 4:  5.1657e-04 -1.8825e-03  2e-03  1e-16  8e-18\n",
            " 5: -7.4023e-07 -2.2145e-04  2e-04  2e-16  2e-18\n",
            " 6: -4.7436e-05 -6.7271e-05  2e-05  2e-16  2e-17\n",
            " 7: -5.1677e-05 -5.3506e-05  2e-06  2e-16  1e-17\n",
            " 8: -5.2064e-05 -5.2233e-05  2e-07  2e-16  7e-18\n",
            " 9: -5.2095e-05 -5.2112e-05  2e-08  1e-16  7e-18\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  2.5193e-01 -6.0325e+01  6e+01  4e-17  1e-15\n",
            " 1:  2.2980e-01 -1.7107e+00  2e+00  2e-16  1e-15\n",
            " 2:  6.0094e-02 -1.9699e-01  3e-01  2e-16  1e-16\n",
            " 3:  6.2488e-03 -2.0740e-02  3e-02  1e-16  8e-17\n",
            " 4:  5.5264e-04 -1.8445e-03  2e-03  2e-16  3e-17\n",
            " 5:  3.6090e-05 -1.8387e-04  2e-04  2e-16  2e-17\n",
            " 6: -1.0248e-05 -2.9862e-05  2e-05  2e-16  3e-18\n",
            " 7: -1.4384e-05 -1.6213e-05  2e-06  2e-16  2e-18\n",
            " 8: -1.4730e-05 -1.4883e-05  2e-07  2e-16  2e-17\n",
            " 9: -1.4755e-05 -1.4768e-05  1e-08  1e-16  4e-18\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  2.5308e-01 -6.0330e+01  6e+01  5e-17  2e-15\n",
            " 1:  2.3078e-01 -1.7144e+00  2e+00  2e-16  1e-15\n",
            " 2:  6.0293e-02 -1.9750e-01  3e-01  2e-16  9e-17\n",
            " 3:  6.2744e-03 -2.0768e-02  3e-02  2e-16  2e-16\n",
            " 4:  5.6339e-04 -1.8400e-03  2e-03  2e-16  2e-17\n",
            " 5:  4.5884e-05 -1.7421e-04  2e-04  2e-16  9e-18\n",
            " 6: -4.4353e-07 -2.0000e-05  2e-05  2e-16  1e-17\n",
            " 7: -4.5476e-06 -6.3796e-06  2e-06  2e-16  7e-18\n",
            " 8: -4.8765e-06 -5.0226e-06  1e-07  2e-16  5e-18\n",
            " 9: -4.9017e-06 -4.9131e-06  1e-08  2e-16  3e-18\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  2.5369e-01 -6.0333e+01  6e+01  4e-17  2e-15\n",
            " 1:  2.3131e-01 -1.7168e+00  2e+00  2e-16  1e-15\n",
            " 2:  6.0406e-02 -1.9780e-01  3e-01  2e-16  1e-16\n",
            " 3:  6.2874e-03 -2.0787e-02  3e-02  1e-16  7e-17\n",
            " 4:  5.6726e-04 -1.8400e-03  2e-03  2e-16  1e-17\n",
            " 5:  4.9064e-05 -1.7115e-04  2e-04  2e-16  3e-18\n",
            " 6:  2.7153e-06 -1.6815e-05  2e-05  2e-16  6e-18\n",
            " 7: -1.3764e-06 -3.2101e-06  2e-06  2e-16  7e-18\n",
            " 8: -1.6969e-06 -1.8388e-06  1e-07  2e-16  1e-17\n",
            " 9: -1.7221e-06 -1.7332e-06  1e-08  2e-16  7e-18\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  2.5402e-01 -6.0335e+01  6e+01  5e-17  2e-15\n",
            " 1:  2.3159e-01 -1.7181e+00  2e+00  2e-16  1e-15\n",
            " 2:  6.0467e-02 -1.9796e-01  3e-01  2e-16  1e-16\n",
            " 3:  6.2941e-03 -2.0798e-02  3e-02  1e-16  2e-16\n",
            " 4:  5.6878e-04 -1.8406e-03  2e-03  1e-16  1e-17\n",
            " 5:  5.0171e-05 -1.7010e-04  2e-04  2e-16  7e-18\n",
            " 6:  3.8052e-06 -1.5710e-05  2e-05  2e-16  1e-18\n",
            " 7: -2.8044e-07 -2.1151e-06  2e-06  2e-16  2e-19\n",
            " 8: -5.9654e-07 -7.3593e-07  1e-07  2e-16  2e-18\n",
            " 9: -6.2175e-07 -6.3267e-07  1e-08  2e-16  2e-19\n",
            "Optimal solution found.\n",
            "optimal solution for x (using modified newton's method):  [[ 0.01541018]\n",
            " [ 0.47777128]\n",
            " [ 0.56820588]\n",
            " [ 0.64997736]\n",
            " [ 0.72687672]\n",
            " [ 0.79377636]\n",
            " [ 0.85377934]\n",
            " [ 0.90288307]\n",
            " [ 0.94336749]\n",
            " [ 0.97252952]\n",
            " [ 0.99172556]\n",
            " [ 0.99893013]\n",
            " [ 0.99673375]\n",
            " [ 0.98312159]\n",
            " [ 0.9581929 ]\n",
            " [ 0.92360665]\n",
            " [ 0.87776735]\n",
            " [ 0.82372428]\n",
            " [ 0.75897894]\n",
            " [ 0.68784091]\n",
            " [ 0.60701925]\n",
            " [ 0.52189562]\n",
            " [ 0.42852976]\n",
            " [ 0.3331403 ]\n",
            " [ 0.2313112 ]\n",
            " [ 0.12982525]\n",
            " [ 0.02398304]\n",
            " [-0.07916384]\n",
            " [-0.18439357]\n",
            " [-0.28469343]]\n",
            "minimum value of F(x) : 0.18750074960247495\n",
            "norm of grad(F(x)) : 6.99235635780034e-05\n",
            "number of iterations :  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "minimum value of F(x) = 0.r*f1(x) + (1-0.r)*f2(x) is 0.18"
      ],
      "metadata": {
        "id": "5Yuez-JZOxbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#q6\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg\n",
        "import pandas as pd\n",
        "import mpmath as mpm\n",
        "\n",
        "delta = 0.01\n",
        "maxIter = 5000\n",
        "r0 = 0.50\n",
        "beta1 = 1e-4\n",
        "beta2 = 0.9\n",
        "epsilon = 1e-4\n",
        "lmbda = 0.001\n",
        "\n",
        "pi = np.pi\n",
        "\n",
        "n = 8\n",
        "file = 'diabetes2.csv'\n",
        "df = pd.read_csv(file)\n",
        "A = df.iloc[:, :n].to_numpy()\n",
        "y = df.iloc[:, n].to_numpy().reshape(-1, 1)\n",
        "A = A.T\n",
        "m = A.shape[1]\n",
        "# print(A.shape)\n",
        "\n",
        "minmax_val = np.zeros((n, 2))\n",
        "for i in range(n):\n",
        "    minmax_val[i][0] = np.min(A[i, :])\n",
        "    minmax_val[i][1] = np.max(A[i, :])\n",
        "\n",
        "    if minmax_val[i][1] > minmax_val[i][0]:\n",
        "        normalized_row = (A[i, :] - minmax_val[i][0]) / (minmax_val[i][1] - minmax_val[i][0])\n",
        "        A[i, :] = normalized_row\n",
        "\n",
        "\n",
        "def p(params):\n",
        "    dp = np.dot(A.T, params)\n",
        "    # pfun = np.zeros((m, 1))\n",
        "    # for i in range(m):\n",
        "    #     pfun[i][0] = 1/(1+mpm.exp(mpm.mpf(dp[i][0])))\n",
        "    # return pfun\n",
        "\n",
        "    return 1/(1+np.exp(dp))\n",
        "\n",
        "\n",
        "def lossFun(params):\n",
        "    pf = p(params)\n",
        "    val = 0.0\n",
        "    for i in range(m):\n",
        "        if y[i][0] == 0:\n",
        "            val += np.log(1 - pf[i][0])\n",
        "        if y[i][0] == 1:\n",
        "            val += np.log(pf[i][0])\n",
        "\n",
        "    val *= (-1/m)\n",
        "\n",
        "\n",
        "    # add L2 regularization..\n",
        "    val += (0.5*lmbda*linalg.norm(params))\n",
        "\n",
        "    return val\n",
        "\n",
        "def gradFx(params):\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "    F = lossFun(params)\n",
        "\n",
        "    h, g = 1e-5, np.zeros(numParams)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        f = lossFun(params)\n",
        "        g[i] = (f - F)/h\n",
        "        params[i][0] -= h\n",
        "\n",
        "    return g.reshape(-1, 1)\n",
        "\n",
        "def hessianFx(params):\n",
        "    h = 1e-5\n",
        "    numParams = params.shape[0]\n",
        "    H = np.matrix(np.zeros((numParams, numParams)))\n",
        "    F = lossFun(params)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        fxi = lossFun(params)\n",
        "\n",
        "        for j in range(i+1):\n",
        "            params[j][0] += h\n",
        "\n",
        "            params[i][0] -= h\n",
        "            fxj = lossFun(params)\n",
        "            params[i][0] += h\n",
        "\n",
        "            H[i,j] = (lossFun(params) - fxi - fxj + F)/(h**2)\n",
        "            H[j,i] = H[i,j]\n",
        "            params[j][0] -= h\n",
        "        params[i][0] -= h\n",
        "\n",
        "    '''\n",
        "    #adding the regularization parameter..\n",
        "    lmbda = min(np.linalg.eig(H)[0])\n",
        "    if lmbda > delta:\n",
        "        lmbda = 0\n",
        "    else:\n",
        "        lmbda = delta-lmbda\n",
        "    for i in range(numParams):\n",
        "        H[i,i] += lmbda\n",
        "\n",
        "    '''\n",
        "\n",
        "    return H\n",
        "\n",
        "\n",
        "def armijo(xk, alphak, dk, gradFxk):\n",
        "    lhs = lossFun(xk + alphak*dk)\n",
        "    rhs = lossFun(xk) + alphak*beta1*np.dot(gradFxk.T, dk)\n",
        "    if lhs <= rhs:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def wolfe(xk, alphak, dk, gradFxk):\n",
        "    lhs = np.dot(gradFx(xk + alphak*dk).T, dk)\n",
        "    rhs = beta2*np.dot(gradFxk.T, dk)\n",
        "    if lhs >= rhs:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def eval(yt):\n",
        "    score = 0\n",
        "    for i in range(m):\n",
        "        if round(yt[i][0]) == y[i][0]:\n",
        "            score += 1\n",
        "        # print(yt[i][0], y[i][0])\n",
        "    print(\"accuracy :\", score/m)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def steepest_descent(params, step):\n",
        "    print(\"...RUNNING STEEPEST DESCENT...\")\n",
        "\n",
        "    iter = 0\n",
        "    xk = params\n",
        "    gradFxk = gradFx(xk)\n",
        "\n",
        "    while linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "        if iter%step == 0:\n",
        "            print(\"iteration \", iter)\n",
        "            print(\"gradient norm \", linalg.norm(gradFxk))\n",
        "\n",
        "        dk = -1 * gradFxk\n",
        "\n",
        "        alpha = 1.00\n",
        "        while armijo(xk, alpha, dk, gradFxk) == False or wolfe(xk, alpha, dk, gradFxk):\n",
        "            alpha = alpha*r0\n",
        "\n",
        "\n",
        "        xk = xk + alpha*dk\n",
        "        gradFxk = gradFx(xk)\n",
        "        iter += 1\n",
        "\n",
        "    print(\"STEEPEST DESCENT METHOD\")\n",
        "    print(\"number of iterations :\", iter)\n",
        "    print(\"optimal solution for x :\", xk)\n",
        "    print(\"minimum value of loss function :\", lossFun(xk))\n",
        "\n",
        "    eval(p(xk))\n",
        "\n",
        "\n",
        "def mirror_descent(params, step):\n",
        "    print(\"...RUNNING MIRROR DESCENT...\")\n",
        "\n",
        "    Q = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(i+1):\n",
        "            if i == j:\n",
        "                Q[i, i] = 0.1\n",
        "            else:\n",
        "                Q[i, j] = 0\n",
        "                Q[j, i] = Q[i, j]\n",
        "\n",
        "\n",
        "    iter = 0\n",
        "    xk = params\n",
        "    gradFxk = gradFx(xk)\n",
        "\n",
        "    while linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "        if iter%step == 0 :\n",
        "            print(\"iteration \", iter)\n",
        "            print(\"gradient norm \", linalg.norm(gradFxk))\n",
        "\n",
        "        dk = -1 * linalg.solve(Q, gradFxk)\n",
        "\n",
        "        alpha = 1.00\n",
        "        while armijo(xk, alpha, dk, gradFxk) == False or wolfe(xk, alpha, dk, gradFxk):\n",
        "            alpha = alpha*r0\n",
        "\n",
        "        xk = xk + alpha*dk\n",
        "        gradFxk = gradFx(xk)\n",
        "        iter += 1\n",
        "\n",
        "    print(\"MIRROR DESCENT METHOD\")\n",
        "    print(\"number of iterations :\", iter)\n",
        "    print(\"optimal solution for x :\", xk)\n",
        "    print(\"minimum value of loss function :\", lossFun(xk))\n",
        "\n",
        "    eval(p(xk))\n",
        "\n",
        "\n",
        "\n",
        "def newton_method(params):\n",
        "    print(\"...RUNNING NEWTON'S METHOD...\")\n",
        "\n",
        "    iter = 0\n",
        "    xk = params\n",
        "    gradFxk = gradFx(xk)\n",
        "    hessFxk = hessianFx(xk)\n",
        "    # A = np.vstack((-np.identity(numParams), np.identity(numParams)))\n",
        "\n",
        "    while linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "        # b = np.append(xk-(-1.0), 1.0-xk)\n",
        "        # b[0] += (-1.0)\n",
        "        # b[0] -= 0.001\n",
        "\n",
        "        # sol = solvers.qp(matrix(hessFxk, tc = 'd'), matrix(gradFxk), matrix(A), matrix(b))\n",
        "        # dk = np.array(sol['x'])\n",
        "        dk = -linalg.solve(hessFxk, gradFxk)\n",
        "        xk = xk + dk\n",
        "\n",
        "        gradFxk = gradFx(xk)\n",
        "        hessFxk = hessianFx(xk)\n",
        "        iter += 1\n",
        "\n",
        "    print(\"NEWTON'S METHOD\")\n",
        "    print(\"number of iterations :\", iter)\n",
        "    print(\"optimal solution for x :\", xk)\n",
        "    print(\"minimum value of loss function :\", lossFun(xk))\n",
        "\n",
        "    eval(p(xk))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    params = np.zeros((n, 1))\n",
        "\n",
        "\n",
        "    steepest_descent(params, 100)\n",
        "    mirror_descent(params, 50)\n",
        "    newton_method(params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuv-IVtnNrla",
        "outputId": "6a676b34-fea7-43d4-a791-1a0aa9f0f0af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...RUNNING STEEPEST DESCENT...\n",
            "iteration  0\n",
            "gradient norm  0.11664373522255006\n",
            "iteration  100\n",
            "gradient norm  0.015586474773004457\n",
            "iteration  200\n",
            "gradient norm  0.00888373841092037\n",
            "iteration  300\n",
            "gradient norm  0.005519488195418069\n",
            "iteration  400\n",
            "gradient norm  0.003602805683407784\n",
            "iteration  500\n",
            "gradient norm  0.0024418876207242183\n",
            "iteration  600\n",
            "gradient norm  0.0017091827682138097\n",
            "iteration  700\n",
            "gradient norm  0.0012303896584561584\n",
            "iteration  800\n",
            "gradient norm  0.0009071589960853367\n",
            "iteration  900\n",
            "gradient norm  0.0006820613823052073\n",
            "iteration  1000\n",
            "gradient norm  0.000520748829318494\n",
            "iteration  1100\n",
            "gradient norm  0.000402224789137417\n",
            "iteration  1200\n",
            "gradient norm  0.0003133334292387571\n",
            "iteration  1300\n",
            "gradient norm  0.00024558753033763635\n",
            "iteration  1400\n",
            "gradient norm  0.00019333077828472954\n",
            "iteration  1500\n",
            "gradient norm  0.00015266511959072989\n",
            "iteration  1600\n",
            "gradient norm  0.00012081921333134171\n",
            "STEEPEST DESCENT METHOD\n",
            "number of iterations : 1682\n",
            "optimal solution for x : [[-0.913806  ]\n",
            " [-1.86684265]\n",
            " [ 4.17499973]\n",
            " [-0.24719476]\n",
            " [-0.7899205 ]\n",
            " [ 0.17369083]\n",
            " [-0.90702028]\n",
            " [-1.63607361]]\n",
            "minimum value of loss function : 0.6049104061465048\n",
            "accuracy : 0.6848958333333334\n",
            "\n",
            "\n",
            "...RUNNING MIRROR DESCENT...\n",
            "iteration  0\n",
            "gradient norm  0.11664373522255006\n",
            "iteration  50\n",
            "gradient norm  0.006080535497468617\n",
            "iteration  100\n",
            "gradient norm  0.0011352515177189676\n",
            "iteration  150\n",
            "gradient norm  0.0003375545872022816\n",
            "iteration  200\n",
            "gradient norm  0.00011414106273919526\n",
            "MIRROR DESCENT METHOD\n",
            "number of iterations : 206\n",
            "optimal solution for x : [[-0.91381399]\n",
            " [-1.86701446]\n",
            " [ 4.17537881]\n",
            " [-0.24727914]\n",
            " [-0.78980989]\n",
            " [ 0.17348165]\n",
            " [-0.90704073]\n",
            " [-1.63616406]]\n",
            "minimum value of loss function : 0.6049103594470324\n",
            "accuracy : 0.6848958333333334\n",
            "\n",
            "\n",
            "...RUNNING NEWTON'S METHOD...\n",
            "NEWTON'S METHOD\n",
            "number of iterations : 4\n",
            "optimal solution for x : [[-0.90815307]\n",
            " [-1.86155469]\n",
            " [ 4.17752547]\n",
            " [-0.25676079]\n",
            " [-0.77536482]\n",
            " [ 0.16195796]\n",
            " [-0.90246819]\n",
            " [-1.63998809]]\n",
            "minimum value of loss function : 0.6049096474603962\n",
            "accuracy : 0.68359375\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Values are normalized using minmax normalization to prevent\n",
        "overflow errors while calculating e^x and divide by zero errors while calculating log(x) and log(1-x)...\n",
        "\n",
        "\n",
        "Steepest Gradient runs in 1600 iterations..\n",
        "Mirror Descent runs in 200 iterations..\n",
        "Newton Method runs in 3-4 iterations..\n",
        "\n",
        "Loss function is minimized at 0.60\n",
        "Using the optimal weights (x_1, x_2, .. , x_n), accuracy obtained is around 70%..."
      ],
      "metadata": {
        "id": "ypjWsbsUPbge"
      }
    }
  ]
}